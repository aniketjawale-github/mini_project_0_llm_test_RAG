# mini_project_0_llm_test_RAG
A Python experiment that demonstrates why LLMs hallucinate without external knowledge and why Retrieval-Augmented Generation (RAG) is necessary.
